# ============================================================================
# GOOGLE COLAB - TRENING MODELU ABSA
# Wymaga: data.zip z folderem data/ zawierającym training_data.json i validation_data.json
# ============================================================================

print("=" * 80)
print("🚀 TRENING MODELU ABSA - GOOGLE COLAB")
print("=" * 80)

# ============================================================================
# KROK 1: INSTALACJA BIBLIOTEK
# ============================================================================
print("\n📦 KROK 1/4: Instalacja bibliotek...")
!pip install -q transformers torch datasets accelerate sacremoses

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup
import json
import numpy as np
from tqdm import tqdm
import os
import zipfile
from typing import Dict, List, Tuple
from google.colab import files
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("✅ Biblioteki zainstalowane!")

# ============================================================================
# KROK 2: SPRAWDZENIE GPU
# ============================================================================
print("\n🖥️  KROK 2/4: Sprawdzenie GPU...")
print(f"GPU dostępne: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"✅ GPU: {torch.cuda.get_device_name(0)}")
    print(f"💾 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
else:
    print("⚠️  Brak GPU - włącz w Runtime → Change runtime type → T4 GPU")
    print("⚠️  ZATRZYMUJĘ - bez GPU trening będzie trwał godziny!")
    raise SystemExit("Włącz GPU i uruchom ponownie")

# ============================================================================
# KROK 3: UPLOAD I ROZPAKOWANIE DANYCH
# ============================================================================
print("\n📤 KROK 3/4: Upload danych")
print("=" * 80)
print("📁 INSTRUKCJA:")
print("   1. Kliknij 'Choose Files' poniżej")
print("   2. Wybierz plik data.zip (folder data/ z training_data.json i validation_data.json)")
print("   3. Poczekaj na upload")
print("=" * 80)

uploaded = files.upload()

if not uploaded:
    print("❌ Nie załadowano żadnego pliku!")
    raise SystemExit("Uruchom ponownie i załaduj data.zip")

# Znajdź ZIP
zip_filename = None
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        zip_filename = filename
        break

if not zip_filename:
    print("❌ Nie znaleziono pliku ZIP!")
    raise SystemExit("Załaduj plik .zip z danymi")

print(f"\n📂 Rozpakowywanie {zip_filename}...")
with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall('.')

# Sprawdź czy są pliki
if not os.path.exists('./data/training_data.json') or not os.path.exists('./data/validation_data.json'):
    print("❌ Błąd: Brak wymaganych plików w ZIP!")
    print("   Struktura ZIP powinna być:")
    print("   data.zip")
    print("   └── data/")
    print("       ├── training_data.json")
    print("       └── validation_data.json")
    raise SystemExit("Popraw strukturę ZIP i spróbuj ponownie")

print("✅ Dane rozpakowane pomyślnie!")
print(f"   📄 {os.path.abspath('./data/training_data.json')}")
print(f"   📄 {os.path.abspath('./data/validation_data.json')}")

# ============================================================================
# KROK 4: KONFIGURACJA
# ============================================================================
print("\n⚙️  KROK 4/4: Konfiguracja i trening")

class Config:
    # Model
    MODEL_NAME = "allegro/herbert-base-cased"
    MAX_LENGTH = 256
    BATCH_SIZE = 32

    # Trening
    LEARNING_RATE = 1.5e-5
    EPOCHS = 25
    WEIGHT_DECAY = 0.01
    WARMUP_RATIO = 0.1

    # Gradient
    GRADIENT_ACCUMULATION_STEPS = 1
    MAX_GRAD_NORM = 1.0

    # Aspekty
    ASPECTS = ["jedzenie", "cena", "obsługa", "atmosfera"]
    NUM_CLASSES_PER_ASPECT = 5

    # Ścieżki
    DATA_DIR = "./data"
    MODEL_DIR = "./models/bert_absa_restaurant"
    TRAINING_FILE = f"{DATA_DIR}/training_data.json"
    VALIDATION_FILE = f"{DATA_DIR}/validation_data.json"

    # GPU
    DEVICE = torch.device("cuda")

    # Early stopping
    PATIENCE = 8
    MIN_DELTA = 0.001

    # Mixed precision
    USE_AMP = True

config = Config()

# ============================================================================
# DATASET
# ============================================================================

class RestaurantReviewDataset(Dataset):
    def __init__(self, data: List[Dict], tokenizer, max_length: int):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        encoding = self.tokenizer(
            item["text"],
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        labels = torch.tensor([
            item["labels"]["jedzenie"] - 1,
            item["labels"]["cena"] - 1,
            item["labels"]["obsługa"] - 1,
            item["labels"]["atmosfera"] - 1
        ], dtype=torch.long)

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": labels
        }

# ============================================================================
# MODEL
# ============================================================================

class BERTABSAModel(nn.Module):
    def __init__(self, model_name: str, num_aspects: int, num_classes: int):
        super(BERTABSAModel, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        hidden_size = self.bert.config.hidden_size
        self.dropout = nn.Dropout(0.2)

        self.aspect_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden_size, 256),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(256, num_classes)
            )
            for _ in range(num_aspects)
        ])

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)

        aspect_logits = []
        for classifier in self.aspect_classifiers:
            logits = classifier(pooled_output)
            aspect_logits.append(logits)

        return aspect_logits

# ============================================================================
# TRAINER
# ============================================================================

class ABSATrainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model.to(config.DEVICE)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config

        # Optimizer
        self.optimizer = AdamW(
            model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )

        # Scheduler
        total_steps = len(train_loader) * config.EPOCHS // config.GRADIENT_ACCUMULATION_STEPS
        warmup_steps = int(total_steps * config.WARMUP_RATIO)

        self.scheduler = get_linear_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )

        self.criterion = nn.CrossEntropyLoss()
        self.scaler = torch.amp.GradScaler('cuda') if config.USE_AMP else None

        # Tracking
        self.best_val_acc = 0.0
        self.patience_counter = 0
        self.training_history = []

    def train_epoch(self) -> float:
        self.model.train()
        total_loss = 0

        self.optimizer.zero_grad()

        progress_bar = tqdm(self.train_loader, desc="Training")
        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch["input_ids"].to(self.config.DEVICE)
            attention_mask = batch["attention_mask"].to(self.config.DEVICE)
            labels = batch["labels"].to(self.config.DEVICE)

            if self.scaler:
                with torch.amp.autocast('cuda'):
                    aspect_logits = self.model(input_ids, attention_mask)
                    loss = 0
                    for i, logits in enumerate(aspect_logits):
                        loss += self.criterion(logits, labels[:, i])
                    loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS

                self.scaler.scale(loss).backward()

                if (batch_idx + 1) % self.config.GRADIENT_ACCUMULATION_STEPS == 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.MAX_GRAD_NORM)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
            else:
                aspect_logits = self.model(input_ids, attention_mask)
                loss = 0
                for i, logits in enumerate(aspect_logits):
                    loss += self.criterion(logits, labels[:, i])
                loss = loss / self.config.GRADIENT_ACCUMULATION_STEPS
                loss.backward()

                if (batch_idx + 1) % self.config.GRADIENT_ACCUMULATION_STEPS == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.MAX_GRAD_NORM)
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()

            total_loss += loss.item() * self.config.GRADIENT_ACCUMULATION_STEPS
            progress_bar.set_postfix({"loss": f"{loss.item() * self.config.GRADIENT_ACCUMULATION_STEPS:.4f}"})

        return total_loss / len(self.train_loader)

    def validate(self) -> Tuple[float, Dict]:
        self.model.eval()
        total_loss = 0
        all_predictions = {aspect: [] for aspect in self.config.ASPECTS}
        all_labels = {aspect: [] for aspect in self.config.ASPECTS}

        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="Validation", leave=False):
                input_ids = batch["input_ids"].to(self.config.DEVICE)
                attention_mask = batch["attention_mask"].to(self.config.DEVICE)
                labels = batch["labels"].to(self.config.DEVICE)

                if self.scaler:
                    with torch.amp.autocast('cuda'):
                        aspect_logits = self.model(input_ids, attention_mask)
                else:
                    aspect_logits = self.model(input_ids, attention_mask)

                loss = 0
                for i, (logits, aspect) in enumerate(zip(aspect_logits, self.config.ASPECTS)):
                    aspect_labels = labels[:, i]
                    loss += self.criterion(logits, aspect_labels)

                    preds = torch.argmax(torch.softmax(logits, dim=1), dim=1)
                    all_predictions[aspect].extend(preds.cpu().numpy())
                    all_labels[aspect].extend(aspect_labels.cpu().numpy())

                total_loss += loss.item()

        avg_loss = total_loss / len(self.val_loader)

        metrics = {}
        overall_accuracy = []
        overall_f1 = []

    # NOWOŚĆ: Oblicz Precision, Recall, F1
        for aspect in self.config.ASPECTS:
            preds = np.array(all_predictions[aspect])
            labels_arr = np.array(all_labels[aspect])

            accuracy = (preds == labels_arr).mean()

        # Precision, Recall, F1 (weighted avg)
            precision, recall, f1, _ = precision_recall_fscore_support(
                labels_arr, preds, average='weighted', zero_division=0
            )

            metrics[aspect] = {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "confusion_matrix": confusion_matrix(labels_arr, preds).tolist()
            }

            overall_accuracy.append(accuracy)
            overall_f1.append(f1)

        metrics["overall_accuracy"] = np.mean(overall_accuracy)
        metrics["overall_f1"] = np.mean(overall_f1)

        return avg_loss, metrics

    def train(self):
        print("\n" + "=" * 80)
        print("🚀 ROZPOCZYNAM TRENING")
        print("=" * 80)
        print(f"🖥️  Urządzenie: {self.config.DEVICE}")
        print(f"⚡ Mixed Precision: {'ON' if self.scaler else 'OFF'}")
        print(f"📊 Batch size: {self.config.BATCH_SIZE}")
        print(f"📊 Epochs: {self.config.EPOCHS}")
        print(f"📊 Learning Rate: {self.config.LEARNING_RATE}")
        print(f"📊 Warmup Ratio: {self.config.WARMUP_RATIO}")
        print(f"📊 Patience: {self.config.PATIENCE}")
        print("=" * 80)

        for epoch in range(self.config.EPOCHS):
            print(f"\n{'='*80}")
            print(f"📅 EPOCH {epoch + 1}/{self.config.EPOCHS}")
            print(f"{'='*80}")

            train_loss = self.train_epoch()
            print(f"\n📉 Training Loss: {train_loss:.4f}")

            val_loss, val_metrics = self.validate()
            print(f"📈 Validation Loss: {val_loss:.4f}")
            print(f"🎯 Overall Accuracy: {val_metrics['overall_accuracy']:.4f}")
            print(f"📊 Overall F1-Score: {val_metrics['overall_f1']:.4f}")

            print("\n   Per-aspect metrics:")
            for aspect in self.config.ASPECTS:
                acc = val_metrics[aspect]['accuracy']
                f1 = val_metrics[aspect]['f1']
                prec = val_metrics[aspect]['precision']
                rec = val_metrics[aspect]['recall']
                print(f"      {aspect:12s}: Acc={acc:.4f} | F1={f1:.4f} | P={prec:.4f} | R={rec:.4f}")

        # Historia
            self.training_history.append({
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "val_loss": val_loss,
                "val_accuracy": val_metrics['overall_accuracy'],
                "val_f1": val_metrics['overall_f1'],
                "aspect_metrics": {
                    aspect: {
                        "accuracy": val_metrics[aspect]['accuracy'],
                        "f1": val_metrics[aspect]['f1'],
                        "precision": val_metrics[aspect]['precision'],
                        "recall": val_metrics[aspect]['recall']
                    }
                    for aspect in self.config.ASPECTS
                }
            })

        # Early stopping - teraz patrzymy na F1
            if val_metrics['overall_f1'] > self.best_val_acc + self.config.MIN_DELTA:
                self.best_val_acc = val_metrics['overall_f1']
                self.patience_counter = 0
                self.save_model()
                print("\n💾 ✨ MODEL SAVED! (New best F1-Score)")
            else:
                self.patience_counter += 1
                print(f"\n⏳ Patience: {self.patience_counter}/{self.config.PATIENCE}")

                if self.patience_counter >= self.config.PATIENCE:
                    print(f"\n🛑 Early stopping triggered")
                    break

        print(f"\n{'='*80}")
        print(f"✅ TRENING ZAKOŃCZONY!")
        print(f"🎯 Best F1-Score: {self.best_val_acc:.4f}")
        print(f"{'='*80}")
        self.save_training_history()

    # NOWOŚĆ: Zapisz confusion matrices
        self.save_confusion_matrices(val_metrics)

# DODAJ FUNKCJĘ:

    def save_confusion_matrices(self, metrics: Dict):
        """Zapisz confusion matrices jako JSON"""
        cm_data = {}
        for aspect in self.config.ASPECTS:
            cm_data[aspect] = metrics[aspect]['confusion_matrix']

        os.makedirs(self.config.MODEL_DIR, exist_ok=True)
        with open(f"{self.config.MODEL_DIR}/confusion_matrices.json", 'w') as f:
            json.dump(cm_data, f, indent=2)

        print(f"\n💾 Confusion matrices zapisane!")

    def save_model(self):
        os.makedirs(self.config.MODEL_DIR, exist_ok=True)
        torch.save(self.model.state_dict(), f"{self.config.MODEL_DIR}/model.pth")

    def save_training_history(self):
        os.makedirs(self.config.MODEL_DIR, exist_ok=True)
        with open(f"{self.config.MODEL_DIR}/training_history.json", 'w') as f:
            json.dump(self.training_history, f, indent=2)

# ============================================================================
# MAIN - PRZYGOTOWANIE I TRENING
# ============================================================================

def load_data(file_path: str) -> List[Dict]:
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def main():
    # Seed
    torch.manual_seed(42)
    np.random.seed(42)
    torch.cuda.manual_seed_all(42)

    # Ładowanie danych
    print("\n📂 Ładowanie danych...")
    train_data = load_data(config.TRAINING_FILE)
    val_data = load_data(config.VALIDATION_FILE)

    print(f"📊 Training samples: {len(train_data)}")
    print(f"📊 Validation samples: {len(val_data)}")

    # Tokenizer
    print(f"\n🔤 Ładowanie tokenizera: {config.MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)

    # Datasety
    train_dataset = RestaurantReviewDataset(train_data, tokenizer, config.MAX_LENGTH)
    val_dataset = RestaurantReviewDataset(val_data, tokenizer, config.MAX_LENGTH)

    # DataLoadery
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=2,
        pin_memory=True
    )

    # Zapisz tokenizer
    os.makedirs(config.MODEL_DIR, exist_ok=True)
    tokenizer.save_pretrained(config.MODEL_DIR)

    # Model
    print(f"\n🤖 Inicjalizacja modelu...")
    model = BERTABSAModel(
        model_name=config.MODEL_NAME,
        num_aspects=len(config.ASPECTS),
        num_classes=config.NUM_CLASSES_PER_ASPECT
    )

    total_params = sum(p.numel() for p in model.parameters())
    print(f"📊 Parametry modelu: {total_params:,}")

    # Trening
    trainer = ABSATrainer(model, train_loader, val_loader, config)
    trainer.train()

    # Pobieranie modelu
    print("\n" + "=" * 80)
    print("📥 POBIERANIE WYTRENOWANEGO MODELU")
    print("=" * 80)

    print("📦 Pakowanie modelu...")
    with zipfile.ZipFile('trained_model.zip', 'w') as zipf:
        for root, dirs, files_list in os.walk(config.MODEL_DIR):
            for file in files_list:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(config.MODEL_DIR))
                zipf.write(file_path, arcname)

    print("📥 Rozpoczynam pobieranie...")
    files.download('trained_model.zip')

    print("\n" + "=" * 80)
    print("🎉 WSZYSTKO GOTOWE!")
    print("=" * 80)
    print("✅ Model wytrenowany")
    print("✅ trained_model.zip pobrane")
    print("\nMożesz zamknąć Colab lub uruchomić ponownie z nowymi danymi.")

if __name__ == "__main__":
    main()